{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UcOPoLTScu4V"
   },
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Adjust the number of parent calls based on the nesting level\n",
    "root_path = str(Path(os.getcwd()).resolve().parent)  \n",
    "sys.path.append(root_path)\n",
    "\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trained_model/nips_1935_iteration_100.pkl\", 'rb') as f:\n",
    "    loaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_topic_word_distribution(model):\n",
    "#     phi = model.calculate_phi_update()  # This gives you the word-topic matrix\n",
    "\n",
    "#     for topic_idx in range(model.num_topics):\n",
    "#         print(f\"Topic #{topic_idx+1}:\\n\")\n",
    "        \n",
    "#         for word_id in range(model.vocab_size):\n",
    "#             word_probability = phi[topic_idx, word_id]\n",
    "#             word = model.id2word[word_id]\n",
    "#             print(f\"{word}: {word_probability:.4f}\")\n",
    "        \n",
    "#         print(\"\\n\\n\")  # Print a newline to separate topics\n",
    "\n",
    "# print_topic_word_distribution(loaded_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< Topic # 1 >>\n",
      "david: 0.0085\n",
      "michael: 0.0074\n",
      "andrew: 0.0063\n",
      "john: 0.0056\n",
      "alex: 0.0054\n",
      "geoffrey: 0.0049\n",
      "peter: 0.0047\n",
      "richard: 0.0045\n",
      "thomas: 0.0041\n",
      "ilya: 0.0039\n",
      "\n",
      "\n",
      "<< Topic # 2 >>\n",
      "node: 0.0043\n",
      "binary: 0.0039\n",
      "graph: 0.0038\n",
      "assign: 0.0038\n",
      "group: 0.0036\n",
      "edge: 0.0035\n",
      "capture: 0.0033\n",
      "identify: 0.0032\n",
      "connect: 0.0032\n",
      "partition: 0.0029\n",
      "\n",
      "\n",
      "<< Topic # 3 >>\n",
      "layer: 0.0057\n",
      "architecture: 0.0055\n",
      "deep: 0.0054\n",
      "bengio: 0.0052\n",
      "hinton: 0.0051\n",
      "convolutional: 0.0043\n",
      "sutskever: 0.0041\n",
      "unit: 0.0039\n",
      "activation: 0.0035\n",
      "lecun: 0.0034\n",
      "\n",
      "\n",
      "<< Topic # 4 >>\n",
      "bayesian: 0.0038\n",
      "posterior: 0.0037\n",
      "likelihood: 0.0036\n",
      "noise: 0.0031\n",
      "inference: 0.0030\n",
      "variance: 0.0030\n",
      "dynamic: 0.0029\n",
      "simulation: 0.0027\n",
      "fit: 0.0024\n",
      "equation: 0.0024\n",
      "\n",
      "\n",
      "<< Topic # 5 >>\n",
      "iid: 0.0040\n",
      "sense: 0.0034\n",
      "family: 0.0033\n",
      "finite: 0.0033\n",
      "uniform: 0.0031\n",
      "turn: 0.0031\n",
      "literature: 0.0029\n",
      "establish: 0.0029\n",
      "implies: 0.0029\n",
      "distance: 0.0028\n",
      "\n",
      "\n",
      "<< Topic # 6 >>\n",
      "convex: 0.0076\n",
      "descent: 0.0062\n",
      "minimization: 0.0057\n",
      "norm: 0.0049\n",
      "regularization: 0.0045\n",
      "dual: 0.0044\n",
      "convexity: 0.0043\n",
      "smooth: 0.0040\n",
      "regularize: 0.0039\n",
      "program: 0.0038\n",
      "\n",
      "\n",
      "<< Topic # 7 >>\n",
      "kernel: 0.0043\n",
      "effective: 0.0039\n",
      "classification: 0.0036\n",
      "http: 0.0036\n",
      "scalable: 0.0035\n",
      "implementation: 0.0034\n",
      "evaluation: 0.0034\n",
      "regression: 0.0033\n",
      "largescale: 0.0032\n",
      "jordan: 0.0030\n",
      "\n",
      "\n",
      "<< Topic # 8 >>\n",
      "cvpr: 0.0055\n",
      "recognition: 0.0053\n",
      "visual: 0.0053\n",
      "vision: 0.0048\n",
      "object: 0.0042\n",
      "human: 0.0039\n",
      "pixel: 0.0039\n",
      "pattern: 0.0038\n",
      "scene: 0.0037\n",
      "image: 0.0037\n",
      "\n",
      "\n",
      "<< Topic # 9 >>\n",
      "online: 0.0037\n",
      "decision: 0.0036\n",
      "bad: 0.0034\n",
      "upper: 0.0033\n",
      "receive: 0.0028\n",
      "arbitrary: 0.0027\n",
      "action: 0.0027\n",
      "regret: 0.0025\n",
      "simply: 0.0025\n",
      "say: 0.0025\n",
      "\n",
      "\n",
      "<< Topic # 10 >>\n",
      "entry: 0.0055\n",
      "sparse: 0.0049\n",
      "spectral: 0.0049\n",
      "norm: 0.0048\n",
      "eigenvalue: 0.0044\n",
      "decomposition: 0.0044\n",
      "column: 0.0043\n",
      "rank: 0.0042\n",
      "recovery: 0.0039\n",
      "row: 0.0038\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_top_words_per_topic(model, top_n=10):\n",
    "    phi = model.calculate_phi_update()  # This gives you the word-topic matrix\n",
    "\n",
    "    for topic_idx in range(model.num_topics):\n",
    "        print(f\"<< Topic # {topic_idx+1} >>\")\n",
    "\n",
    "        # Get the top N word indices for the topic sorted by probability\n",
    "        top_word_indices = phi[topic_idx].argsort()[-top_n:][::-1]\n",
    "        \n",
    "        for word_id in top_word_indices:\n",
    "            word_probability = phi[topic_idx, word_id]\n",
    "            word = model.id2word[word_id]\n",
    "            print(f\"{word}: {word_probability:.4f}\")\n",
    "\n",
    "        print(\"\\n\")  # Print a newline to separate topics\n",
    "\n",
    "# After running your model...\n",
    "print_top_words_per_topic(loaded_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract word for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you've run Gibbs sampling\n",
    "word_topic_matrix = loaded_model.word_topic_matrix\n",
    "word_topic_sum = word_topic_matrix.sum(axis=1)[:, np.newaxis]\n",
    "word_topic_dist = word_topic_matrix / word_topic_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the top N words for each topic\n",
    "N_TOP_WORDS = 50\n",
    "\n",
    "ALL_TOPIC_WORDS = []\n",
    "for i in range(loaded_model.num_topics):\n",
    "    top_words_idx = word_topic_dist[i].argsort()[-N_TOP_WORDS:][::-1]\n",
    "    top_words = [loaded_model.id2word[idx] for idx in top_words_idx]\n",
    "\n",
    "    ALL_TOPIC_WORDS.append(top_words)\n",
    "\n",
    "    # print(f\"Topic {i + 1}: {', '.join(top_words)} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dic = {}\n",
    "\n",
    "for topic_words in ALL_TOPIC_WORDS:\n",
    "  for word in topic_words:\n",
    "    if word in word_dic:\n",
    "      word_dic[word] = word_dic[word] + 1\n",
    "    else:\n",
    "      word_dic[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the author-topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the author_topic_matrix to get author-topic distribution\n",
    "\n",
    "# Compute the sum of rows in author_topic_matrix\n",
    "author_topic_sum = loaded_model.author_topic_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Replace zero sums with a small epsilon value\n",
    "epsilon = 1e-10\n",
    "author_topic_sum[author_topic_sum == 0] = epsilon\n",
    "\n",
    "# Perform element-wise division\n",
    "author_topic_dist = loaded_model.author_topic_matrix / author_topic_sum\n",
    "\n",
    "# Visualize the top N topics for each author\n",
    "N_TOP_TOPICS = 3\n",
    "top_topics_list = []\n",
    "for i, author in enumerate(loaded_model.authors):\n",
    "    top_topics_idx = author_topic_dist[i].argsort()[-N_TOP_TOPICS:][::-1]\n",
    "    top_topics_list.append(top_topics_idx)\n",
    "    # print(f\"Author {i+1} => {author} : Topic IDs {top_topics_idx} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sebastian Stober</td>\n",
       "      <td>[2, 3, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daniel J. Cameron</td>\n",
       "      <td>[9, 8, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jessica A. Grahn</td>\n",
       "      <td>[9, 8, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aurel A. Lazar</td>\n",
       "      <td>[3, 4, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yevgeniy Slutskiy</td>\n",
       "      <td>[9, 8, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chen-Yu Wei</td>\n",
       "      <td>[8, 4, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yi-Te Hong</td>\n",
       "      <td>[9, 8, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chi-Jen Lu</td>\n",
       "      <td>[9, 8, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Katherine A. Heller</td>\n",
       "      <td>[2, 3, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>David B. Dunson</td>\n",
       "      <td>[3, 4, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Xiangyu Wang</td>\n",
       "      <td>[3, 4, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Fangjian Guo</td>\n",
       "      <td>[9, 8, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Lars Buesing</td>\n",
       "      <td>[3, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>David Silver</td>\n",
       "      <td>[0, 8, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Daan Wierstra</td>\n",
       "      <td>[9, 8, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Nicolas Heess</td>\n",
       "      <td>[3, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Oriol Vinyals</td>\n",
       "      <td>[2, 0, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Razvan Pascanu</td>\n",
       "      <td>[2, 7, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Danilo Jimenez Rezende</td>\n",
       "      <td>[3, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Theophane Weber</td>\n",
       "      <td>[9, 8, 7]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   authors     topics\n",
       "0         Sebastian Stober  [2, 3, 1]\n",
       "1        Daniel J. Cameron  [9, 8, 7]\n",
       "2         Jessica A. Grahn  [9, 8, 7]\n",
       "3           Aurel A. Lazar  [3, 4, 9]\n",
       "4        Yevgeniy Slutskiy  [9, 8, 7]\n",
       "5              Chen-Yu Wei  [8, 4, 5]\n",
       "6               Yi-Te Hong  [9, 8, 7]\n",
       "7               Chi-Jen Lu  [9, 8, 7]\n",
       "8      Katherine A. Heller  [2, 3, 5]\n",
       "9          David B. Dunson  [3, 4, 9]\n",
       "10            Xiangyu Wang  [3, 4, 6]\n",
       "11            Fangjian Guo  [9, 8, 7]\n",
       "12            Lars Buesing  [3, 0, 2]\n",
       "13            David Silver  [0, 8, 3]\n",
       "14           Daan Wierstra  [9, 8, 7]\n",
       "15           Nicolas Heess  [3, 2, 0]\n",
       "16           Oriol Vinyals  [2, 0, 7]\n",
       "17          Razvan Pascanu  [2, 7, 3]\n",
       "18  Danilo Jimenez Rezende  [3, 2, 0]\n",
       "19         Theophane Weber  [9, 8, 7]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_topics_of_authors_df = pd.DataFrame({'authors': loaded_model.authors, 'topics': top_topics_list})\n",
    "top_topics_of_authors_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>Hongyuan Zha</td>\n",
       "      <td>[9, 6, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          authors     topics\n",
       "655  Hongyuan Zha  [9, 6, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_topics_of_authors_df[top_topics_of_authors_df['authors'] == 'Hongyuan Zha']\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "UcOPoLTScu4V",
    "S_BBNjjzc4m5",
    "c0cAeBowGUVP"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lda-implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1669881b8e0ee381f1d44208a6e6b4675430ed382f288976bd9acdbb8db18405"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
